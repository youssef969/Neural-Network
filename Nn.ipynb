{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class Activition_functions():\n",
    "\n",
    "    @staticmethod\n",
    "    def Tanh(z):\n",
    "        return (math.exp(z) - math.exp(-z)) / (math.exp(z) + math.exp(-z))\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + math.exp(-x))\n",
    "    \n",
    "    @staticmethod\n",
    "    def Relu(value):\n",
    "        return max(0,value)\n",
    "    \n",
    "    @staticmethod\n",
    "    def Linear(value):\n",
    "        return value\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid_derivative(x):\n",
    "        return x * (1 - x)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as rn\n",
    "import numpy as np \n",
    "\n",
    "class Neural_network():\n",
    "    def __init__(self,input_layer:int,Hidden_layers:list,output_layer:int,activition_function:Activition_functions,alpha:float):\n",
    "        self.output_layer = output_layer\n",
    "        self.input_layer = input_layer\n",
    "        self.Hidden_layers = Hidden_layers\n",
    "        self.activition_function = activition_function\n",
    "        self.alpha = alpha\n",
    "        self.index = 0\n",
    "        self.derivitive_activition_function =  self.__Backpropigation_activition_function()\n",
    "        \n",
    "\n",
    "    def __Backpropigation_activition_function(self):\n",
    "        active_func_name =  self.activition_function.__name__\n",
    "        if  active_func_name == \"sigmoid\":\n",
    "            return Activition_functions.sigmoid_derivative\n",
    "\n",
    "    def __Hidden_layer_nerouns(self):\n",
    "        if self.index < len(self.Hidden_layers):\n",
    "            element = self.Hidden_layers[self.index]\n",
    "            self.index += 1\n",
    "            return element\n",
    "        else:\n",
    "            return None  \n",
    "\n",
    "    def Input_layer(self,input_layer:dict | None ) -> dict :\n",
    "        if input_layer == None:\n",
    "            input_layer = {f\"i{i}\": rn.uniform(-0.5,0.5) for i in range(self.input_layer)}\n",
    "            print(f\"Intial Values to Input Layer = {input_layer}\")\n",
    "            return input_layer\n",
    "        else:\n",
    "            print(f\"Intial Values to Input Layer = {input_layer}\")\n",
    "            return input_layer\n",
    "    \n",
    "        \n",
    "    \n",
    "    def Hidden_layer(self,weghits:list,input_layer:dict,bais:float):\n",
    "        \"\"\"weights here is Weights from input Neuron to Next Neuron\"\"\"\n",
    "\n",
    "        Nerons = self.__Hidden_layer_nerouns()\n",
    "        hidden_layer = {f\"h{i}\": 0 for i in range(Nerons)}\n",
    "        pathes = Nerons * len(input_layer)\n",
    "        inputs = np.array(list(input_layer.values()))\n",
    "        \n",
    "        if pathes != len(weghits):\n",
    "            raise ValueError(\"Logical Error: Number of pathes not uqual number of weights\")\n",
    "        elif Nerons == None:\n",
    "            raise ValueError(\"Logical Error: Error\")\n",
    "        else: \n",
    "            inputs = inputs.reshape(1,len(input_layer))\n",
    "            weights_matrix = np.array(weghits).reshape(len(input_layer),Nerons)\n",
    "            result = np.dot(inputs ,weights_matrix)\n",
    "            result = result + bais \n",
    "            for index , Value in zip([x for x in range(Nerons)],result[0].tolist()):\n",
    "                hidden_layer[f\"h{index}\"] = self.activition_function(Value)\n",
    "        print(f\"Hidden Layer: {Nerons} Nerouns Values = {hidden_layer}\")\n",
    "        return hidden_layer\n",
    "\n",
    "        \n",
    "    def Output_layer(self,weghits:list,Hidden_layer:dict,bais:float) -> dict:\n",
    "        \"\"\"weights here is Weights from input Neuron to Next Neuron\"\"\"\n",
    "\n",
    "        output_layer = {f\"o{i}\": 0 for i in range(self.output_layer)}\n",
    "        pathes = len(Hidden_layer) * self.output_layer\n",
    "        if pathes != len(weghits):\n",
    "            raise ValueError(\"Logical Error: Number of pathes not uqual number of weights\")\n",
    "        else:\n",
    "            inputs = np.array(list(Hidden_layer.values())).reshape(1,len(Hidden_layer))\n",
    "            weights_matrix = np.array(weghits).reshape(len(Hidden_layer),len(output_layer))      \n",
    "            result = np.dot(inputs,weights_matrix,)\n",
    "            result = result + bais\n",
    "            for index , Value in zip([x for x in range(self.output_layer)],result[0].tolist()):\n",
    "                output_layer[f\"o{index}\"] = self.activition_function(Value)\n",
    "        print(f\"Output Layer Nerouns Values = {output_layer}\")\n",
    "        return output_layer\n",
    "    \n",
    "    def Total_error(self,targets:list,output_layer:dict):\n",
    "        error = 0\n",
    "        for target , output in zip(targets,list(output_layer.values())):\n",
    "           error = error  +  0.5*(target - output)**2\n",
    "        print(f\"Error Rate = {error * 100} %\")\n",
    "        return error\n",
    "    \n",
    "    def Backpropagation(self, input_layer:dict, hidden_layer:dict, output_layer:dict, weights_hidden:list, weights_output:list, targets:list):\n",
    "        input_layer = np.array(list(input_layer.values()))\n",
    "        hidden_layer = np.array(list(hidden_layer.values()))\n",
    "        output_layer = np.array(list(output_layer.values())) \n",
    "        targets = np.array(targets)\n",
    "        \n",
    "        #! output Layer update Weight\n",
    "        alpha_output = -1*(targets - output_layer) * self.derivitive_activition_function(output_layer)\n",
    "        delta_output =alpha_output * hidden_layer * self.alpha\n",
    "        output_updated_weights = np.array(weights_output).reshape(2,self.output_layer) - delta_output\n",
    "        output_updated_weights_list = list(output_updated_weights.flatten())\n",
    "        print([f\"{y} Updated to: {x}\" for x,y in zip(output_updated_weights_list , weights_output)])\n",
    "        \n",
    "\n",
    "        #! Hidden Layer update Weight\n",
    "        alpha_hidden_layer = np.dot(alpha_output,np.array(weights_output).reshape(2,2)) * self.derivitive_activition_function(hidden_layer) \n",
    "        delta_hideen_layer = alpha_hidden_layer * self.alpha * input_layer\n",
    "        hidden_updated_weights = np.array(weights_hidden).reshape(2,self.output_layer) - delta_hideen_layer\n",
    "        hidden_updated_weights_list = list(hidden_updated_weights.flatten())\n",
    "        print([f\"{y} Updated to: {x}\" for x,y in zip(hidden_updated_weights_list, weights_hidden)])\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "       \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intial Values to Input Layer = {'i1': 0.1, 'i2': 0.5}\n",
      "Hidden Layer: 2 Nerouns Values = {'h0': 0.6010878788483698, 'h1': 0.6153837563911821}\n",
      "Output Layer Nerouns Values = {'o0': 0.7349286127170142, 'o1': 0.7795538841677558}\n",
      "Error Rate = 50.06880372113564 %\n",
      "['0.5 Updated to: 0.4518781254241036', '0.7 Updated to: 0.653708287867778', '0.6 Updated to: 0.5518781254241036', '0.8 Updated to: 0.7537082878677781']\n",
      "['0.1 Updated to: 0.09795793982957476', '0.2 Updated to: 0.18624615559968657', '0.3 Updated to: 0.29795793982957475', '0.4 Updated to: 0.38624615559968656']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nn = Neural_network(input_layer=2,Hidden_layers=[2],output_layer=2,activition_function=Activition_functions.sigmoid,alpha=0.6)\n",
    "\n",
    "\n",
    "input_layer = nn.Input_layer(input_layer={\"i1\":0.1,\"i2\":0.5})\n",
    "hidden_layer = nn.Hidden_layer(input_layer=input_layer,weghits=[0.1,0.2,0.3,0.4],bais=0.25)\n",
    "output_layer = nn.Output_layer(weghits=[0.5,0.7,0.6,0.8],Hidden_layer=hidden_layer,bais=0.35,)\n",
    "total_error = nn.Total_error(targets=[0.05,0.05],output_layer=output_layer)\n",
    "\n",
    "b = nn.Backpropagation(\n",
    "    input_layer=input_layer,\n",
    "    hidden_layer=hidden_layer,\n",
    "    output_layer=output_layer,\n",
    "    weights_hidden=[0.1,0.2,0.3,0.4],\n",
    "    weights_output=[0.5,0.7,0.6,0.8],\n",
    "    targets=[0.05,0.05]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stuart 5\n",
      "4\n",
      "Stuart 8\n",
      "Stuart 10\n",
      "5\n",
      "Stuart 10\n"
     ]
    }
   ],
   "source": [
    "# https://www.hackerrank.com/challenges/the-minion-game/problem\n",
    "def minion_game(string):\n",
    "    l = len(string)\n",
    "    Stuart,Kevin = 0,0\n",
    "    for i in range(l):\n",
    "        if string[i] in \"AEIOU\":\n",
    "            Kevin+=l-i\n",
    "            print(Kevin)\n",
    "        else:\n",
    "            Stuart+=l-i\n",
    "            print(f\"Stuart {Stuart}\")\n",
    "    \n",
    "    print(f\"Stuart {Stuart}\" if Stuart>Kevin else f\"Kevin {Kevin}\" if Kevin>Stuart else \"Draw\")\n",
    "    \n",
    "                \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    s = \"HELLO\"\n",
    "    minion_game(s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
