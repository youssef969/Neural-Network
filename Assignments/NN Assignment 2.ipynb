{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class Activition_functions():\n",
    "    @staticmethod\n",
    "    def Tanh(z):\n",
    "        return (math.exp(z) - math.exp(-z)) / (math.exp(z) + math.exp(-z))\n",
    "    @staticmethod\n",
    "    def sigmoid(x):\n",
    "        \n",
    "        return 1 / (1 + math.exp(-x))\n",
    "    @staticmethod\n",
    "    def Relu(value):\n",
    "        return max(0,value)\n",
    "    \n",
    "    def Linear(value):\n",
    "        return value\n",
    "    \n",
    "    def sigmoid_derivative(x):\n",
    "        return x * (1 - x)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as rn\n",
    "import numpy as np \n",
    "\n",
    "class Neural_network():\n",
    "    def __init__(self,input_layer:int,Hidden_layers:list,output_layer:int,activition_function:Activition_functions,alpha:float):\n",
    "        self.output_layer = output_layer\n",
    "        self.input_layer = input_layer\n",
    "        self.Hidden_layers = Hidden_layers\n",
    "        self.activition_function = activition_function\n",
    "        self.alpha = alpha\n",
    "        self.index = 0\n",
    "        self.derivitive_activition_function =  self.__Backpropigation_activition_function()\n",
    "        \n",
    "\n",
    "    def __Backpropigation_activition_function(self):\n",
    "        active_func_name =  self.activition_function.__name__\n",
    "        if  active_func_name == \"sigmoid\":\n",
    "            return Activition_functions.sigmoid_derivative\n",
    "\n",
    "    def __Hidden_layer_nerouns(self):\n",
    "        if self.index < len(self.Hidden_layers):\n",
    "            element = self.Hidden_layers[self.index]\n",
    "            self.index += 1\n",
    "            return element\n",
    "        else:\n",
    "            return None  \n",
    "\n",
    "    def Input_layer(self,input_layer:dict | None ) -> dict :\n",
    "        if input_layer == None:\n",
    "            input_layer = {f\"i{i}\": rn.uniform(-0.5,0.5) for i in range(self.input_layer)}\n",
    "            print(f\"Intial Values to Input Layer = {input_layer}\")\n",
    "            return input_layer\n",
    "        else:\n",
    "            print(f\"Intial Values to Input Layer = {input_layer}\")\n",
    "            return input_layer\n",
    "    \n",
    "        \n",
    "    \n",
    "    def Hidden_layer(self,weghits:list,input_layer:dict,bais:float):\n",
    "        \"\"\"weights here is Weights from input Neuron to Next Neuron\"\"\"\n",
    "\n",
    "        Nerons = self.__Hidden_layer_nerouns()\n",
    "        hidden_layer = {f\"h{i}\": 0 for i in range(Nerons)}\n",
    "        pathes = Nerons * len(input_layer)\n",
    "        inputs = np.array(list(input_layer.values()))\n",
    "        \n",
    "        if pathes != len(weghits):\n",
    "            raise ValueError(\"Logical Error: Number of pathes not uqual number of weights\")\n",
    "        elif Nerons == None:\n",
    "            raise ValueError(\"Logical Error: Error\")\n",
    "        else: \n",
    "            inputs = inputs.reshape(1,len(input_layer))\n",
    "            weights_matrix = np.array(weghits).reshape(len(input_layer),Nerons)\n",
    "            result = np.dot(inputs ,weights_matrix)\n",
    "            result = result + bais \n",
    "            for index , Value in zip([x for x in range(Nerons)],result[0].tolist()):\n",
    "                hidden_layer[f\"h{index}\"] = self.activition_function(Value)\n",
    "        print(f\"Hidden Layer: {Nerons} Nerouns Values = {hidden_layer}\")\n",
    "        return hidden_layer\n",
    "\n",
    "        \n",
    "    def Output_layer(self,weghits:list,Hidden_layer:dict,bais:float) -> dict:\n",
    "        \"\"\"weights here is Weights from input Neuron to Next Neuron\"\"\"\n",
    "\n",
    "        output_layer = {f\"o{i}\": 0 for i in range(self.output_layer)}\n",
    "        pathes = len(Hidden_layer) * self.output_layer\n",
    "        if pathes != len(weghits):\n",
    "            raise ValueError(\"Logical Error: Number of pathes not uqual number of weights\")\n",
    "        else:\n",
    "            inputs = np.array(list(Hidden_layer.values())).reshape(1,len(Hidden_layer))\n",
    "            weights_matrix = np.array(weghits).reshape(len(Hidden_layer),len(output_layer))      \n",
    "            result = np.dot(inputs,weights_matrix,)\n",
    "            result = result + bais\n",
    "            for index , Value in zip([x for x in range(self.output_layer)],result[0].tolist()):\n",
    "                output_layer[f\"o{index}\"] = self.activition_function(Value)\n",
    "        print(f\"Output Layer Nerouns Values = {output_layer}\")\n",
    "        return output_layer\n",
    "    \n",
    "    def Total_error(self,targets:list,output_layer:dict):\n",
    "        error = 0\n",
    "        for target , output in zip(targets,list(output_layer.values())):\n",
    "           error = error  +  0.5*(target - output)**2\n",
    "        print(f\"Error Rate = {error * 100} %\")\n",
    "        return error\n",
    "    \n",
    "    def Backpropagation(self, input_layer:dict, hidden_layer:dict, output_layer:dict, weights_hidden:list, weights_output:list, targets:list):\n",
    "        input_layer = np.array(list(input_layer.values()))\n",
    "        hidden_layer = np.array(list(hidden_layer.values()))\n",
    "        output_layer = np.array(list(output_layer.values())) \n",
    "        targets = np.array(targets)\n",
    "        \n",
    "        #! output Layer update Weight\n",
    "        alpha_output = -1*(targets - output_layer) * self.derivitive_activition_function(output_layer)\n",
    "        delta_output =alpha_output * hidden_layer * self.alpha\n",
    "        output_updated_weights = np.array(weights_output).reshape(2,self.output_layer) - delta_output\n",
    "        output_updated_weights_list = list(output_updated_weights.flatten())\n",
    "        print([f\"{y} Updated to: {x}\" for x,y in zip(output_updated_weights_list , weights_output)])\n",
    "        \n",
    "\n",
    "        #! Hidden Layer update Weight\n",
    "        alpha_hidden_layer = np.dot(alpha_output,np.array(weights_output).reshape(2,2)) * self.derivitive_activition_function(hidden_layer) \n",
    "        delta_hideen_layer = alpha_hidden_layer * self.alpha * input_layer\n",
    "        hidden_updated_weights = np.array(weights_hidden).reshape(2,self.output_layer) - delta_hideen_layer\n",
    "        hidden_updated_weights_list = list(hidden_updated_weights.flatten())\n",
    "        print([f\"{y} Updated to: {x}\" for x,y in zip(hidden_updated_weights_list, weights_hidden)])\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "       \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intial Values to Input Layer = {'i1': 0.05, 'i2': 0.1}\n",
      "Hidden Layer: 2 Nerouns Values = {'h0': np.float64(0.5932699921071872), 'h1': np.float64(0.596884378259767)}\n",
      "Output Layer Nerouns Values = {'o0': np.float64(0.7513650695523157), 'o1': np.float64(0.7729284653214625)}\n",
      "Error Rate = 29.837110876000274 %\n",
      "['0.4 Updated to: 0.35891647971788465', '0.5 Updated to: 0.5113701211079891', '0.45 Updated to: 0.40891647971788464', '0.55 Updated to: 0.5613701211079891']\n",
      "['0.15 Updated to: 0.14976922471842932', '0.25 Updated to: 0.24941897573424526', '0.2 Updated to: 0.19976922471842934', '0.3 Updated to: 0.29941897573424525']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nn = Neural_network(input_layer=2,Hidden_layers=[2],output_layer=2,activition_function=Activition_functions.sigmoid,alpha=0.5)\n",
    "\n",
    "\n",
    "input_layer = nn.Input_layer(input_layer={\"i1\":0.05,\"i2\":0.10})\n",
    "hidden_layer = nn.Hidden_layer(input_layer=input_layer,weghits=[0.15,0.25,0.20,0.30],bais=0.35)\n",
    "output_layer = nn.Output_layer(weghits=[0.40,0.50,0.45,0.55],Hidden_layer=hidden_layer,bais=0.60,)\n",
    "total_error = nn.Total_error(targets=[0.01,0.99],output_layer=output_layer)\n",
    "\n",
    "b = nn.Backpropagation(\n",
    "    input_layer=input_layer,\n",
    "    hidden_layer=hidden_layer,\n",
    "    output_layer=output_layer,\n",
    "    weights_hidden=[0.15, 0.25, 0.20, 0.30],\n",
    "    weights_output=[0.40, 0.50, 0.45, 0.55],\n",
    "    targets=[0.01, 0.99]\n",
    ")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
